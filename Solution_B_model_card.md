---
{}
---
language: en

license: cc-by-4.0

tags:
- text-classification

repo: https://github.com/Sasha-XIONG/NLI

---

# Model Card for v08387yx-NLI

<!-- Provide a quick summary of what the model is/does. -->

This is a multilayer perceptron that was trained to identify whether a “hypothesis” is true (entailment) or false (non-entailment) given a “premise”.


## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

This model involves 9 layers, including 1 input layer, 2 linear layer, 2 dropout layer, 2 batch normalisation layer and 1 output layer.
ReLU and Sigmoid are selected as the activation functions.
It was trained on more than 26K premise-hypothesis pairs and validated on less than 7K pairs.
It uses embeddings generated by bert-base-uncased model (which does not make a difference between english and English) as input features,
and predicts the relationship between a hypothesis and a premise as 0 (non-entailment) or 1 (entailment).

- **Developed by:** Yu Xiong
- **Language(s):** English
- **Model type:** Supervised
- **Model architecture:** Multilayer Perceptron (MLP)
- **Feature extraction with model:** bert-base-uncased
- **Finetuned from model [optional]:** NA

### Model Resources

<!-- Provide links where applicable. -->

- **Repository:** NA
- **Paper or documentation:** NA

## Training Details

### Training Data

<!-- This is a short stub of information on the training data that was used, and documentation related to data pre-processing or additional filtering (if applicable). -->

The entire training set provided, amounting to 26942 premise-hypothesis pairs.

### Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Training Hyperparameters

<!-- This is a summary of the values of hyperparameters used in training the model. -->


      - learning_rate: 2e-03
      - train_batch_size: 32
      - eval_batch_size: 32
      - num_epochs: 10
      - optimiser: Adam
      - loss function: Binary cross entropy loss
      - dropout probability: 0.3

#### Speeds, Sizes, Times

<!-- This section provides information about how roughly how long it takes to train the model and the size of the resulting model. -->


      - overall training time: around 48 minutes
      - duration per training epoch: around 4.8 minutes
      - model size: 912KB

## Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

### Testing Data & Metrics

#### Testing Data

<!-- This should describe any evaluation data used (e.g., the development/validation set provided). -->

The entire development set provided, amounting to 6737 premise-hypothesis pairs.

#### Metrics

<!-- These are the evaluation metrics being used. -->


      - Precision (macro)
      - Recall (macro)
      - F1-score (macro)
      - Accuracy

### Results

The model obtained a macro precision-score of 68.57%, a macro recall-score of 68.54%, an macro F1-score of 68.55% and an accuracy of 68.60%.

## Technical Specifications

### Hardware


      - RAM: at least 40 GB
      - Storage: 225.8GB (26.2 used),
      - GPU: L4

### Software


      - Transformers 4.18.0
      - Pytorch 1.11.0+cu113
      - Pandas 2.0.3
      - Scikit-learn 1.2.2

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

Any inputs (concatenation of two sequences) longer than 128 subwords will be truncated by the model.
It is noteworthy that few samples in the training dataset
contains language other than English (e.g. Greek), this model may perform better with a cleaner training dataset.

## Additional Information

<!-- Any other information that would be useful for other people to know. -->

The hyperparameters were determined by experimentation
      with different values. Note that this model may take a very long time to train if GPU is not available.
