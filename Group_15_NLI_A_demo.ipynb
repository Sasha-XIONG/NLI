{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyO8pTI9LujRDVys4jJ1bBUN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKaHjTR0ColM","executionInfo":{"status":"ok","timestamp":1713835102277,"user_tz":-60,"elapsed":188631,"user":{"displayName":"Naomi Li","userId":"00648983487732438904"}},"outputId":"3332227f-6517-4c55-a284-6834d8a7a03c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","   prediction\n","0           1\n","1           1\n","2           1\n","3           1\n","4           1\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from joblib import dump, load\n","import spacy\n","import torch\n","from transformers import XLMRobertaModel, XLMRobertaTokenizer\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/34812 CW NLI')\n","\n","modela = load('./linear_trained_model.joblib')\n","\n","nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n","tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","model = XLMRobertaModel.from_pretrained('xlm-roberta-base', output_hidden_states=True)\n","\n","def preprocess(text):\n","    doc = nlp(text)\n","    tokens = [\n","        token.lemma_.lower() for token in doc\n","        if not token.is_punct\n","    ]\n","    return ' '.join(tokens)\n","\n","def preprocess_and_encode(df, columns, max_length):\n","    combined_features = []\n","    # Target length for padding/truncation of the token embeddings\n","    target_embedding_length = 768\n","    batch_size = 16\n","\n","    for column in columns:\n","        column_embeddings = []\n","        cleaned_texts = [preprocess(text) for text in df[column].dropna()]\n","\n","        # Target length for padding/truncation of the token embeddings\n","        for i in range(0, len(cleaned_texts), batch_size):\n","            batch_texts = cleaned_texts[i:i + batch_size]\n","            # Tokenize the texts and encode with padding and truncation\n","            encoded_inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n","\n","            # Evaluate the model to get embeddings without gradient updates\n","            model.eval()\n","            with torch.no_grad():\n","                outputs = model(**encoded_inputs)\n","            embeddings = torch.stack(outputs.hidden_states[-4:]).mean(0).mean(dim=1).numpy()\n","\n","            batch_embeddings = np.zeros((embeddings.shape[0], target_embedding_length))\n","            for j, emb in enumerate(embeddings):\n","                actual_length = min(target_embedding_length, emb.shape[0])\n","                batch_embeddings[j, :actual_length] = emb[:actual_length]\n","\n","            column_embeddings.append(batch_embeddings)\n","\n","        column_embeddings = np.vstack(column_embeddings)\n","        combined_features.append(column_embeddings)\n","\n","    return np.hstack(combined_features)\n","\n","def demo(input_data):\n","    input_data['premise'] = input_data['premise'].fillna('').astype(str)\n","    input_data['hypothesis'] = input_data['hypothesis'].fillna('').astype(str)\n","    features = preprocess_and_encode(input_data, ['premise', 'hypothesis'], max_length=128)\n","    predictions = modela.predict(features)\n","    return predictions\n","\n","\n","validation_df = pd.read_csv('./test.csv', keep_default_na=False)\n","results = demo(validation_df)\n","predictions = pd.DataFrame(results, columns=['prediction'])\n","print(predictions.head())\n","output_file_path = './Group_15_A.csv'\n","predictions.to_csv(output_file_path, index=False)\n","\n"]},{"cell_type":"code","source":["# from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n","# y_validation = validation_df['label'].values\n","# print(classification_report(y_validation, results))\n","# print(f\"Accuracy: {accuracy_score(y_validation, results)}\")\n","# print(f\"Precision, Recall, F1 Score: {precision_recall_fscore_support(y_validation, results, average='weighted')[:3]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkLhHcsLiCPE","executionInfo":{"status":"ok","timestamp":1713650615965,"user_tz":-60,"elapsed":327,"user":{"displayName":"Naomi Li","userId":"00648983487732438904"}},"outputId":"e8e3770a-83e8-44b2-c813-fdb37b59769b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.59      0.80      0.68      3259\n","           1       0.72      0.48      0.57      3478\n","\n","    accuracy                           0.63      6737\n","   macro avg       0.65      0.64      0.62      6737\n","weighted avg       0.65      0.63      0.62      6737\n","\n","Accuracy: 0.6320320617485528\n","Precision, Recall, F1 Score: (0.6537482635574783, 0.6320320617485528, 0.6230948973111805)\n"]}]}]}